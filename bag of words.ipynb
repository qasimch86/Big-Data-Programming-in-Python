{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "   \n",
    "1. Representation of text that describes the occuerence of words within a text \n",
    "2. Any information about order or structure is discarded\n",
    "3. Represent document x as set of(wi,fi)\n",
    "\n",
    "We convert text to a numerical represenattion called a feature vector . A feature vector can be as simple as a list of numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements=[{'city':'Dubai','temperature':33.},\n",
    "              {'city':'London','temperature':12.},\n",
    "              {'city':'San Francsisco','temperature':18.},{'city':'Toronto','temperature':14.},]\n",
    "#Transforms list of feature-value \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec=DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0. 33.]\n",
      " [ 0.  1.  0.  0. 12.]\n",
      " [ 0.  0.  1.  0. 18.]\n",
      " [ 0.  0.  0.  1. 14.]]\n"
     ]
    }
   ],
   "source": [
    "print(vec.fit_transform(measurements).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city=Dubai', 'city=London', 'city=San Francsisco', 'city=Toronto', 'temperature']\n"
     ]
    }
   ],
   "source": [
    "print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Methods in DictVectorizer\n",
    "1. fit(X[,y])\tLearn a list of feature name -> indices mappings.\n",
    "2. fit_transform(X[,y])\tLearn a list of feature name -> indices mappings and transform X.\n",
    "3. get_feature_names()\tReturns a list of feature names, ordered by their indices.\n",
    "4. get_params([deep])\tGet parameters for this estimator.\n",
    "5. inverse_transform(X[, dict_type])\tTransform array or sparse matrix X back to feature mappings.\n",
    "6. restrict(support[, indices])\tRestrict the features to those in support using feature selection.\n",
    "7. set_params(**params)\tSet the parameters of this estimator.\n",
    "8. transform(X)\tTransform feature->value dicts to array or sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert a collection of text documents to a matrix of token counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\n",
    "    'Mcmaster BDA 102 is an interesting course',\n",
    "    'I can not wait for Saturday!',\n",
    "    'Mcmaster students shall use BDA 102 to successfully combat Skynet',\n",
    "    'I wish i was as strong as thanos',\n",
    "    'Sarah Conor is out of reach.'\n",
    "]\n",
    "#print(type(corpus))\n",
    "#print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mcmaster BDA 102 is an interesting course', 'I can not wait for Saturday!', 'Mcmaster students shall use BDA 102 to successfully combat Skynet', 'I wish i was as strong as thanos', 'Sarah Conor is out of reach.']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(ngram_range=(3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(3, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "#todense converts to a spare matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mcmaster': 11, 'bda': 3, '102': 0, 'is': 10, 'an': 1, 'interesting': 9, 'course': 7, 'can': 4, 'not': 12, 'wait': 26, 'for': 8, 'saturday': 17, 'students': 21, 'shall': 18, 'use': 25, 'to': 24, 'successfully': 22, 'combat': 5, 'skynet': 19, 'wish': 28, 'was': 27, 'as': 2, 'strong': 20, 'thanos': 23, 'sarah': 16, 'conor': 6, 'out': 14, 'of': 13, 'reach': 15}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Methods in CountVectorizer\n",
    "1. build_analyzer()\tReturn a callable that handles preprocessing and tokenization\n",
    "2. build_preprocessor()\tReturn a function to preprocess the text before tokenization\n",
    "3. build_tokenizer()\tReturn a function that splits a string into a sequence of tokens\n",
    "4. decode(doc)\tDecode the input into a string of unicode symbols\n",
    "5. fit(raw_documents[, y])\tLearn a vocabulary dictionary of all tokens in the raw documents.\n",
    "6. fit_transform(raw_documents[, y])\tLearn the vocabulary dictionary and return term-document matrix.\n",
    "7. get_feature_names()\tArray mapping from feature integer indices to feature name\n",
    "8. get_params([deep])\tGet parameters for this estimator.\n",
    "9. get_stop_words()\tBuild or fetch the effective stop words list\n",
    "10. inverse_transform(X)\tReturn terms per document with nonzero entries in X.\n",
    "11. set_params(**params)\tSet the parameters of this estimator.\n",
    "12. transform(raw_documents)\tTransform documents to document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application\n",
    "train_set=(\"The sky is blue,\", \"The sun is bright.\")\n",
    "test_set=(\" The sun in the sky is bright.\",\n",
    "          \"We can see the shining sun ,the bright sun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sky': 2, 'blue': 0, 'sun': 3, 'bright': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer =CountVectorizer(stop_words='english',encoding='utf-16')\n",
    "vectorizer.fit_transform(train_set)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.transform(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1]\n",
      " [0 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.transform(test_set).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASS ACTIVITY\n",
    "1. go to this avenue select the welcome message from BDA 102 and 101 or BDA 104 OR BDA 103\n",
    "\n",
    "2. create a list with  variables containing each text message \n",
    "\n",
    "3. Build vocubalary then print array from the list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2\n",
    "\n",
    "1. Sum the two rows and build a frequency table\n",
    "\n",
    "2. check type and see whether it is dataframe \n",
    "\n",
    "3. if not dataframe,convert it to a dataframe.\n",
    "\n",
    "4. To this dataframe give a column name "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
